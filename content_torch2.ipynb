{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThis notebook is a Pytorch implementation of the CONTENT model as presented in\\nthe paper \\n\"Readmission prediction via deep contextual embedding of clinical concepts\" \\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890980/pdf/pone.0195024.pdf\\n\\nOriginal code is available at https://github.com/danicaxiao/CONTENT\\n\\nauthors: Gowtham Kuntumalla, Yiming Li\\ndate: April, 2023\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "This notebook is a Pytorch implementation of the CONTENT model as presented in\n",
    "the paper \n",
    "\"Readmission prediction via deep contextual embedding of clinical concepts\" \n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890980/pdf/pone.0195024.pdf\n",
    "\n",
    "Original code is available at https://github.com/danicaxiao/CONTENT\n",
    "\n",
    "authors: Gowtham Kuntumalla, Yiming Li\n",
    "date: April, 2023\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "# futher imports done belo\n",
    "import matplotlib.pyplot as plt\n",
    "from patient_data_reader import PatientReader\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import average_precision_score as pr_auc_score\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "\n",
    "# repeated here for ease of readability\n",
    "class Config:\n",
    "    \"\"\"feel free to play with these hyperparameters during training\"\"\"\n",
    "    dataset = \"resource\"  # change this to the right data name\n",
    "    data_path = \"%s\" % dataset\n",
    "    checkpoint_dir = \"checkpoint\"\n",
    "    decay_rate = 0.95\n",
    "    decay_step = 1000\n",
    "    n_topics = 50\n",
    "    learning_rate = 0.001 # 0.00002\n",
    "    vocab_size = 619\n",
    "    n_stops = 22 \n",
    "    lda_vocab_size = vocab_size - n_stops\n",
    "    n_hidden = 200\n",
    "    n_layers = 2\n",
    "    projector_embed_dim = 100\n",
    "    generator_embed_dim = n_hidden\n",
    "    dropout = 1.0\n",
    "    max_grad_norm = 1.0 #for gradient clipping\n",
    "    total_epoch = 6\n",
    "    init_scale = 0.075\n",
    "    threshold = 0.5 #probability cut-off for predicting label to be 1\n",
    "    forward_only = False #indicates whether we are in testing or training mode\n",
    "    log_dir = 'logs'\n",
    "    \n",
    "    \n",
    "FLAGS = Config()\n",
    "\n",
    "# Define hyperparameters\n",
    "N_HIDDEN = FLAGS.n_hidden\n",
    "NUM_EPOCHS = FLAGS.total_epoch\n",
    "N_VOCAB = FLAGS.vocab_size\n",
    "EMBED_SIZE = FLAGS.projector_embed_dim\n",
    "N_HIDDEN = FLAGS.n_hidden\n",
    "N_TOPICS = FLAGS.n_topics\n",
    "LEARNING_RATE = FLAGS.learning_rate\n",
    "THRESHOLD = FLAGS.threshold\n",
    "\n",
    "N_BATCH = 1#1 # 128 # 128, 32, 1\n",
    "MAX_LENGTH = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# useful for when GPU is available else use CPU as default.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(seqs, labels, vocabsize, maxlen=None):\n",
    "    \"\"\"Create the matrices from the datasets.\n",
    "\n",
    "    This pad each sequence to the same lenght: the length of the\n",
    "    longuest sequence or maxlen.\n",
    "\n",
    "    if maxlen is set, we will cut all sequence to this maximum\n",
    "    lenght.\n",
    "\n",
    "    This swap the axis!\n",
    "    \n",
    "    We modified some parts of this code from the original paper\n",
    "    \"\"\"\n",
    "    # x: a list of sentences\n",
    "    lengths = [len(s) for s in seqs]\n",
    "\n",
    "    eventSeq = []\n",
    "\n",
    "    for seq in seqs:\n",
    "        t = []\n",
    "        for visit in seq:\n",
    "            t.extend(visit)\n",
    "        eventSeq.append(t)\n",
    "    eventLengths = [len(s) for s in eventSeq]\n",
    "\n",
    "    if maxlen is not None:\n",
    "        new_seqs = []\n",
    "        new_lengths = []\n",
    "        new_labels = []\n",
    "        for l, s, la in zip(lengths, seqs, labels):\n",
    "            if l < maxlen:\n",
    "                new_seqs.append(s)\n",
    "                new_lengths.append(l)\n",
    "                new_labels.append(la)\n",
    "            else:\n",
    "                new_seqs.append(s[:maxlen])\n",
    "                new_lengths.append(maxlen)\n",
    "                new_labels.append(la[:maxlen])\n",
    "        lengths = new_lengths\n",
    "        seqs = new_seqs\n",
    "        labels = new_labels\n",
    "\n",
    "        if len(lengths) < 1:\n",
    "            return None, None, None\n",
    "\n",
    "    n_samples = len(seqs)\n",
    "    maxlen = max(maxlen, np.max(lengths)) # changed this line to always to goto max_len as we use in pytroch with batches\n",
    "\n",
    "    x = np.zeros((n_samples, maxlen, vocabsize)).astype('int64')\n",
    "    x_mask = np.zeros((n_samples, maxlen)).astype(float)\n",
    "    y = np.ones((n_samples, maxlen)).astype(float)\n",
    "    for idx, s in enumerate(seqs):\n",
    "        x_mask[idx, :lengths[idx]] = 1\n",
    "        for j, sj in enumerate(s):\n",
    "            for tsj in sj:\n",
    "                x[idx, j, tsj - 1] = 1\n",
    "    for idx, t in enumerate(labels):\n",
    "        y[idx, :lengths[idx]] = t\n",
    "        # if lengths[idx] < maxlen:\n",
    "        #     y[idx,lengths[idx]:] = t[-1]\n",
    "    \n",
    "#     # randomly generated list of labels. for testing. note that this size is n_samples,1 and not n_samples,n_visits\n",
    "#     y = torch.randint(0, 2, (n_samples,)) #.astype(float)\n",
    "    return x, x_mask, y, lengths, eventLengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] load resource\\vocab.pkl\n",
      " [*] load resource/X_train.pkl\n",
      " [*] load resource/Y_train.pkl\n",
      " [*] load resource/X_valid.pkl\n",
      " [*] load resource/Y_valid.pkl\n",
      " [*] load resource/X_test.pkl\n",
      " [*] load resource/Y_test.pkl\n",
      "vocabulary size: 619\n",
      "number of training documents: 2000\n",
      "number of validation documents: 500\n",
      "number of testing documents: 500\n",
      "legth of dataset of dtype = train: 2000\n",
      "legth of dataset of dtype = valid: 500\n",
      "legth of dataset of dtype = test: 500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a custom dataset for use in the pytorch pipelines.\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, seqs, hfs):\n",
    "        self.x = seqs\n",
    "        self.y = hfs\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    \n",
    "\n",
    "# This patientreader class helps in reading the raw data and converting it to more amenable format.\n",
    "data_sets = PatientReader(FLAGS)\n",
    "\n",
    "def get_custom_dataset(dtype):\n",
    "    \"\"\" dtype in train, valid, test\"\"\"\n",
    "    X_raw_data, Y_raw_data = data_sets.get_data_from_type(dtype)\n",
    "    dataset = CustomDataset(X_raw_data, Y_raw_data)\n",
    "    print(f\"legth of dataset of dtype = {dtype}:\", len(X_raw_data))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# create train, test and validation\n",
    "train_dataset = get_custom_dataset(\"train\")\n",
    "val_dataset = get_custom_dataset(\"valid\")\n",
    "test_dataset =get_custom_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    function required when visit lenghts are unequal. Masks are the tensors whcih store flags of real visits.\n",
    "    used in dataloader.\n",
    "    \"\"\"\n",
    "    sequences, labels = zip(*data)\n",
    "\n",
    "    x, x_mask, y, lengths, eventLengths = prepare_data(seqs=sequences, labels=labels, vocabsize=N_VOCAB, maxlen=MAX_LENGTH)\n",
    "    \n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    x_mask = torch.tensor(x_mask, dtype=torch.bool)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "    return x, x_mask, y, lengths, eventLengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data.dataset import random_split\n",
    "\n",
    "# split = int(len(dataset)*0.5)\n",
    "\n",
    "# lengths = [split, len(dataset) - split]\n",
    "# train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "# print(\"Length of train dataset:\", len(train_dataset))\n",
    "# print(\"Length of val dataset:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset,test_dataset, collate_fn,batch_size=128):\n",
    "    \n",
    "    '''\n",
    "    Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to batch_size. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        test dataset: test dataset of type `CustomDataset`\n",
    "        \n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader, test_dataset : train and validation and test dataloaders\n",
    "    '''\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset,test_dataset, collate_fn,batch_size = N_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    In RNN, gettin the last hidden state form list of hidden states. \n",
    "    \"\"\"\n",
    "    mask_length = masks.count_nonzero(dim=1)\n",
    "    return hidden_states[range(hidden_states.shape[0]),mask_length-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# input = torch.randn(batch_size, sequence_length, input_size)\n",
    "\n",
    "print_flag = False\n",
    "class GRUModel(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU only model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GRUModel, self).__init__()\n",
    "#         l_embed = lasagne.layers.DenseLayer(l_in, num_units=embedsize, b=None, num_leading_axes=2)\n",
    "        self.embed = nn.Linear(N_VOCAB, EMBED_SIZE, bias=False)\n",
    "        self.gru = nn.GRU(input_size=EMBED_SIZE, hidden_size=N_HIDDEN, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features= N_HIDDEN, out_features=MAX_LENGTH)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, masks):\n",
    "        # forward pass\n",
    "        if print_flag: print( \"x\" ,x.shape)\n",
    "        if print_flag: print( \"masks\",masks.shape)\n",
    "        batch_size = x.shape[0]\n",
    "        if print_flag: print( \"batch_size\", batch_size)\n",
    "        x_embed = self.embed(x)\n",
    "        if print_flag: print( \"x_embed\", x_embed.shape)\n",
    "        output, h_n = self.gru(x_embed)\n",
    "        if print_flag: print( \"output\", output.shape)\n",
    "        if print_flag: print( \"h_n\", h_n.shape)\n",
    "        true_h_n = get_last_visit(output, masks)\n",
    "        if print_flag: print( \"true_h_n\",true_h_n.shape)\n",
    "        logits = self.fc(true_h_n)   \n",
    "        if print_flag: print( \"logits\",logits.shape)\n",
    "        probs = self.sigmoid(logits)\n",
    "        if print_flag: print( \"probs\",probs.shape)\n",
    "        probs_ret = probs.view((batch_size,-1))\n",
    "        if print_flag: print( \"probs_ret\",probs_ret.shape)\n",
    "        return probs_ret\n",
    "    \n",
    "## H0 defaults to zeros if not provided.\n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(1, N_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUModel(\n",
       "  (embed): Linear(in_features=619, out_features=100, bias=False)\n",
       "  (gru): GRU(100, 200, batch_first=True)\n",
       "  (fc): Linear(in_features=200, out_features=300, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_rnn = GRUModel()\n",
    "gru_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 300, 619]), torch.Size([1, 300]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "x, x_mask, y, lengths, eventLengths = next(train_iter)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(gru_rnn.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    evaluate the model. GRU\n",
    "    \"\"\"\n",
    "    def apply_mask(lengths, y):\n",
    "        \"\"\"\n",
    "        for metrics need to somehow for each patient get only real n_visit not all of max_length.\n",
    "            like new_testlabels.extend(inputs[1].flatten()[:leng])\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for i, l in enumerate(lengths):\n",
    "            result.extend(y[i][:l])\n",
    "        return result\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = [] # torch.LongTensor()\n",
    "    y_score = []#torch.Tensor()\n",
    "    y_true = []#torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, x_mask, y, lengths, eventLengths in data_loader:\n",
    "#         print(\"lengths: \", lengths)\n",
    "        y_hat_prob = model(x, x_mask)\n",
    "        y_score.extend(apply_mask(lengths, y_hat_prob.detach().to(device)))\n",
    "        \n",
    "        y_hat = (y_hat_prob > THRESHOLD).int()\n",
    "#         print(np.shape(y_pred), np.shape(y_true))\n",
    "        y_pred.extend(apply_mask(lengths, y_hat.detach().to(device)))\n",
    "        y_true.extend(apply_mask(lengths, y.detach().to(device)))\n",
    "#         print(np.shape(y_pred), np.shape(y_true))\n",
    "#         print(y_pred[:2])\n",
    "        \n",
    "    \"\"\"\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "    \n",
    "#     print(\"after loop: \", np.shape(y_pred), np.shape(y_true))\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    pr_auc = pr_auc_score(y_true, y_score)\n",
    "    return acc, p, r, f, roc_auc, pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    train the model. GRU\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, x_mask, y, lengths, eventLengths in train_loader:\n",
    "            \"\"\"\n",
    "                1. zero grad\n",
    "                2. model forward\n",
    "                3. calculate loss\n",
    "                4. loss backward\n",
    "                5. optimizer step\n",
    "            \"\"\"\n",
    "            outputs = model(x, x_mask)\n",
    "#             print(\"outputs\",outputs.shape)\n",
    "#             print(\"y\",y.shape)\n",
    "            loss = criterion(outputs, y) \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        acc, p, r, f, roc_auc, pr_auc = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation acc: {:.2f}, p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}, pr_auc: {:.2f}'\n",
    "              .format(epoch+1, acc, p, r, f, roc_auc, pr_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train(gru_rnn, train_loader, val_loader, NUM_EPOCHS, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# acc, p, r, f, roc_auc,pr_auc = eval_model(gru_rnn, val_loader)\n",
    "# print('Validation acc: {:.2f}, p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}, pr_auc: {:.2f}'\n",
    "#               .format(acc, p, r, f, roc_auc, pr_auc))\n",
    "\n",
    "\n",
    "# acc, p, r, f, roc_auc,pr_auc  = eval_model(gru_rnn, test_loader)\n",
    "# print('Test acc: {:.2f}, p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}, pr_auc: {:.2f}'\n",
    "#               .format(acc, p, r, f, roc_auc, pr_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CONTENT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score \n",
    "from sklearn.metrics import average_precision_score as pr_auc\n",
    "\n",
    "\n",
    "def eval_model_content(model, data_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    evaluate the model. content\n",
    "\n",
    "    \"\"\"\n",
    "    def apply_mask(lengths, y):\n",
    "        \"\"\"\n",
    "        for metrics need to somehow for each patient get only real n_visit not all of max_length.\n",
    "            like new_testlabels.extend(inputs[1].flatten()[:leng])\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for i, l in enumerate(lengths):\n",
    "            result.extend(y[i][:l])\n",
    "        return result\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = [] # torch.LongTensor()\n",
    "    y_score = []#torch.Tensor()\n",
    "    y_true = []#torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, x_mask, y, lengths, eventLengths in data_loader:\n",
    "#         print(\"lengths: \", lengths)\n",
    "        y_hat_prob, klterm = model(x, x_mask)\n",
    "        y_score.extend(apply_mask(lengths, y_hat_prob.detach().to(device)))\n",
    "        \n",
    "        y_hat = (y_hat_prob > THRESHOLD).int()\n",
    "#         print(np.shape(y_pred), np.shape(y_true))\n",
    "        y_pred.extend(apply_mask(lengths, y_hat.detach().to(device)))\n",
    "        y_true.extend(apply_mask(lengths, y.detach().to(device)))\n",
    "#         print(np.shape(y_pred), np.shape(y_true))\n",
    "#         print(y_pred[:2])\n",
    "        \n",
    "    \"\"\"\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "    \n",
    "#     print(\"after loop: \", np.shape(y_pred), np.shape(y_true))\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    pr_auc = pr_auc_score(y_true, y_score)\n",
    "    return acc, p, r, f, roc_auc, pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_content(model, train_loader, val_loader, n_epochs, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    train the model. using klterm\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, x_mask, y, lengths, eventLengths in train_loader:\n",
    "            \"\"\"\n",
    "                1. zero grad\n",
    "                2. model forward\n",
    "                3. calculate loss\n",
    "                4. loss backward\n",
    "                5. optimizer step\n",
    "            \"\"\"\n",
    "            outputs, klterm = model(x, x_mask)\n",
    "#             print(\"outputs\",outputs.shape)\n",
    "#             print(\"y\",y.shape)\n",
    "            loss = criterion(outputs, y) + klterm\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        acc, p, r, f, roc_auc, pr_auc = eval_model_content(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation acc: {:.2f}, p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}, pr_auc: {:.2f}'\n",
    "              .format(epoch+1, acc, p, r, f, roc_auc, pr_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ThetaLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Part of recognition network. Distribution.\n",
    "    \n",
    "    We didn't find this part of the code in the original author's code repository. \n",
    "    We used openAI's ChatGPT 3.5 to help generate intial version of this and then modified it further. \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_topics, max_length):\n",
    "        super(ThetaLayer, self).__init__()\n",
    "        self.n_topics = n_topics\n",
    "        self.max_length = max_length\n",
    "        self.klterm = None\n",
    "\n",
    "    def forward(self, mu, log_sigma):\n",
    "        self.klterm = -0.5 * torch.sum(1 + 2 * log_sigma - mu.pow(2) - torch.exp(log_sigma).pow(2)) / mu.size(0)\n",
    "        eps = torch.randn(mu.size(0), self.max_length, self.n_topics).to(mu.device)\n",
    "        theta = mu.unsqueeze(1) + torch.exp(log_sigma).unsqueeze(1) * eps # sampling from the normal distribution of mu and sigma\n",
    "        self.theta = nn.Parameter(theta.mean(dim=0, keepdim=False))\n",
    "        return self.theta\n",
    "\n",
    "class CONTENT(nn.Module):\n",
    "    \"\"\"\n",
    "    Full CONTENT model. Note: This implentation is incomplete. More optimization is needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, n_hidden, n_topics, maxlen):\n",
    "        super(CONTENT, self).__init__()\n",
    "        \n",
    "        # embed layer to reduce dimensionality\n",
    "        self.l_embed = nn.Linear(vocab_size, embed_size, bias=False)\n",
    "        \n",
    "        # GRU\n",
    "        self.l_forward0 = nn.GRU(embed_size, n_hidden, batch_first=True, bidirectional=False)\n",
    "\n",
    "        # Recognition Net\n",
    "        self.l_1 = nn.Linear(vocab_size, n_hidden)\n",
    "        self.l_2 = nn.Linear(n_hidden, n_hidden)\n",
    "        \n",
    "        self.mu = nn.Linear(n_hidden, n_topics)\n",
    "        self.log_sigma = nn.Linear(n_hidden, n_topics)\n",
    "        self.l_theta = ThetaLayer(n_topics, maxlen)\n",
    "        \n",
    "        self.l_B = nn.Linear(vocab_size, n_topics, bias=False) \n",
    "        self.l_context = nn.Sequential(\n",
    "            nn.BatchNorm1d(n_topics),\n",
    "            nn.Linear(n_topics, n_topics)\n",
    "        )\n",
    "        self.l_dense0 = nn.Linear(n_hidden, 1)\n",
    "        self.l_dense1 = nn.Flatten(start_dim=1)\n",
    "        self.l_dense = nn.Sequential(\n",
    "            nn.Linear(maxlen, 1),\n",
    "            nn.Flatten(start_dim=1),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # forward pass\n",
    "        printf = False\n",
    "        batch_size = x.shape[0]\n",
    "        if printf: print(\"x\", x.shape)\n",
    "        \n",
    "        x_emb = self.l_embed(x)\n",
    "        if printf: print(\"x_emb\",x_emb.shape)\n",
    "        \n",
    "        x_forward, _ = self.l_forward0(x_emb)\n",
    "        if printf: print(\"mask\",mask.shape)\n",
    "        \n",
    "        x_forward = x_forward * mask.unsqueeze(2)\n",
    "        if printf: print(\"x_forward\",x_forward.shape)\n",
    "\n",
    "        x_1 = F.relu(self.l_1(x))\n",
    "        x_2 = F.relu(self.l_2(x_1))\n",
    "        \n",
    "        # mu, sigma as per paper: batchsize * n_topic\n",
    "        mu = self.mu(x_2)\n",
    "        log_sigma = self.log_sigma(x_2)\n",
    "        if printf: print(\"mu\",mu.shape)\n",
    "        if printf: print(\"log_sigma\",log_sigma.shape)\n",
    "\n",
    "        \n",
    "        theta = self.l_theta(mu, log_sigma) # batchsize * maxlen * n_topic\n",
    "        if printf: print(\"theta\",theta.shape)\n",
    "        klterm = self.l_theta.klterm # to be used in cost calculation\n",
    "        if printf: print(\"klterm\",klterm)\n",
    "        \n",
    "        b = self.l_B(x)\n",
    "        if printf: print(\"b\",b.shape)\n",
    "        \n",
    "        context = b * theta # elem wise multiplication\n",
    "        if printf: print(\"context before layer\",context.shape)\n",
    "#         context = self.l_context(context)\n",
    "#         if printf: print(\"context after layer\",context.shape)\n",
    "        context = torch.mean(context, dim=-1)\n",
    "        if printf: print(\"context\",context.shape)\n",
    "        \n",
    "        # combine GRU and context\n",
    "        x_dense0 = self.l_dense0(x_forward)\n",
    "        if printf: print(\"x_dense0\",x_dense0.shape)\n",
    "        \n",
    "        x_dense1 = self.l_dense1(x_dense0)\n",
    "        if printf: print(\"x_dense1\",x_dense1.shape)\n",
    "#         x_dense = self.l_dense(torch.cat((x_dense1, context), dim=1))\n",
    "\n",
    "        if printf: print(\"x_dense1 + context\",(x_dense1 + context).shape)\n",
    "        x_dense = self.l_dense(x_dense1 + context)\n",
    "        if printf: print(\"x_dense\",x_dense.shape)\n",
    "\n",
    "        output = torch.sigmoid(x_dense) * mask + 0.000001\n",
    "        if printf: print(\"output\",output.shape)\n",
    "#         output = output.view(-1, maxlen)\n",
    "        output = output.view((batch_size,-1))\n",
    "        if printf: print(\"output\",output.shape)\n",
    "        \n",
    "        return output, klterm\n",
    "\n",
    "# Define model and optimizer\n",
    "content_model = CONTENT(N_VOCAB, EMBED_SIZE, N_HIDDEN, N_TOPICS, MAX_LENGTH)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(content_model.parameters(),lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter = iter(train_loader)\n",
    "# x, x_mask, y, lengths, eventLengths = next(train_iter)\n",
    "# x.shape, y.shape\n",
    "\n",
    "# model = content_model\n",
    "# n_epochs = NUM_EPOCHS\n",
    "# for epoch in range(n_epochs):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     for x, x_mask, y, lengths, eventLengths in train_loader:\n",
    "#         \"\"\"\n",
    "#             1. zero grad\n",
    "#             2. model forward\n",
    "#             3. calculate loss\n",
    "#             4. loss backward\n",
    "#             5. optimizer step\n",
    "#         \"\"\"\n",
    "#         outputs, klterm = model(x, x_mask)\n",
    "# #             print(\"outputs\",outputs.shape)\n",
    "# #             print(\"y\",y.shape)\n",
    "#         loss = criterion(outputs, y) + klterm\n",
    "#         # Backward pass\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item()\n",
    "#     train_loss = train_loss / len(train_loader)\n",
    "#     print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "#     p, r, f, roc_auc = eval_model_content(model, val_loader)\n",
    "#     print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "#           .format(epoch+1, p, r, f, roc_auc))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_content(content_model, train_loader, val_loader, NUM_EPOCHS, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc, p, r, f, roc_auc,pr_auc  = eval_model_content(content_model, val_loader)\n",
    "# print('Validation acc: {:.2f}, p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}, pr_auc: {:.2f}'\n",
    "#               .format(acc, p, r, f, roc_auc, pr_auc))\n",
    "\n",
    "# acc, p, r, f, roc_auc,pr_auc  = eval_model_content(content_model, test_loader)\n",
    "# print('Test acc: {:.2f}, p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}, pr_auc: {:.2f}'\n",
    "#               .format(acc, p, r, f, roc_auc, pr_auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Runs and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch: 1 \t Training Loss: 0.241364\n",
      "Epoch: 1 \t Validation acc: 0.72, p: 0.29, r:0.16, f: 0.20, roc_auc: 0.56, pr_auc: 0.26\n",
      "Epoch: 2 \t Training Loss: 0.214809\n",
      "Epoch: 2 \t Validation acc: 0.73, p: 0.32, r:0.18, f: 0.23, roc_auc: 0.59, pr_auc: 0.28\n",
      "Epoch: 3 \t Training Loss: 0.170089\n",
      "Epoch: 3 \t Validation acc: 0.74, p: 0.37, r:0.21, f: 0.26, roc_auc: 0.64, pr_auc: 0.32\n",
      "Epoch: 4 \t Training Loss: 0.158554\n",
      "Epoch: 4 \t Validation acc: 0.75, p: 0.39, r:0.21, f: 0.27, roc_auc: 0.65, pr_auc: 0.33\n",
      "Epoch: 5 \t Training Loss: 0.155061\n",
      "Epoch: 5 \t Validation acc: 0.73, p: 0.36, r:0.23, f: 0.28, roc_auc: 0.64, pr_auc: 0.31\n",
      "Epoch: 6 \t Training Loss: 0.151918\n",
      "Epoch: 6 \t Validation acc: 0.75, p: 0.39, r:0.21, f: 0.27, roc_auc: 0.65, pr_auc: 0.33\n",
      "1\n",
      "Epoch: 1 \t Training Loss: 0.245764\n",
      "Epoch: 1 \t Validation acc: 0.71, p: 0.30, r:0.20, f: 0.24, roc_auc: 0.58, pr_auc: 0.28\n",
      "Epoch: 2 \t Training Loss: 0.176100\n",
      "Epoch: 2 \t Validation acc: 0.73, p: 0.34, r:0.20, f: 0.25, roc_auc: 0.62, pr_auc: 0.31\n",
      "Epoch: 3 \t Training Loss: 0.162459\n",
      "Epoch: 3 \t Validation acc: 0.75, p: 0.38, r:0.20, f: 0.26, roc_auc: 0.64, pr_auc: 0.32\n",
      "Epoch: 4 \t Training Loss: 0.156921\n",
      "Epoch: 4 \t Validation acc: 0.76, p: 0.42, r:0.18, f: 0.25, roc_auc: 0.65, pr_auc: 0.34\n",
      "Epoch: 5 \t Training Loss: 0.154977\n",
      "Epoch: 5 \t Validation acc: 0.76, p: 0.44, r:0.19, f: 0.26, roc_auc: 0.66, pr_auc: 0.35\n",
      "Epoch: 6 \t Training Loss: 0.152393\n",
      "Epoch: 6 \t Validation acc: 0.74, p: 0.38, r:0.22, f: 0.28, roc_auc: 0.64, pr_auc: 0.33\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "results_gru = pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_RUNS):\n",
    "    print(i)\n",
    "    \n",
    "    # GRU\n",
    "    gru_rnn = GRUModel()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(gru_rnn.parameters(), lr=LEARNING_RATE)\n",
    "    train(gru_rnn, train_loader, val_loader, NUM_EPOCHS, criterion, optimizer)\n",
    "    acc, p, r, f, roc_auc,pr_auc  = eval_model(gru_rnn, test_loader)\n",
    "    tmp = pd.DataFrame({'roc_auc':[roc_auc] , 'pr_auc':[pr_auc], 'accuracy':[acc], 'precision':[p], 'recall':[r], 'f1':[f]}).round(2)\n",
    "    results_gru = pd.concat([results_gru,tmp])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_gru)\n",
    "display(results_gru.mean().round(2))\n",
    "display(results_gru.std().round(2))\n",
    "results_gru.to_csv(f\"pytorch_results_gru_epoch{NUM_EPOCHS}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch: 1 \t Training Loss: 10.392436\n",
      "Epoch: 1 \t Validation acc: 0.78, p: 0.54, r:0.11, f: 0.18, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 2 \t Training Loss: 10.287996\n",
      "Epoch: 2 \t Validation acc: 0.78, p: 0.62, r:0.07, f: 0.12, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 3 \t Training Loss: 10.286212\n",
      "Epoch: 3 \t Validation acc: 0.78, p: 0.59, r:0.08, f: 0.14, roc_auc: 0.67, pr_auc: 0.39\n",
      "Epoch: 4 \t Training Loss: 10.285068\n",
      "Epoch: 4 \t Validation acc: 0.78, p: 0.59, r:0.08, f: 0.15, roc_auc: 0.66, pr_auc: 0.37\n",
      "Epoch: 5 \t Training Loss: 10.284655\n",
      "Epoch: 5 \t Validation acc: 0.77, p: 0.49, r:0.17, f: 0.25, roc_auc: 0.67, pr_auc: 0.38\n",
      "Epoch: 6 \t Training Loss: 10.284005\n",
      "Epoch: 6 \t Validation acc: 0.78, p: 0.63, r:0.04, f: 0.07, roc_auc: 0.66, pr_auc: 0.37\n",
      "1\n",
      "Epoch: 1 \t Training Loss: 10.380548\n",
      "Epoch: 1 \t Validation acc: 0.78, p: 0.52, r:0.15, f: 0.23, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 2 \t Training Loss: 10.287967\n",
      "Epoch: 2 \t Validation acc: 0.78, p: 0.54, r:0.09, f: 0.15, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 3 \t Training Loss: 10.286997\n",
      "Epoch: 3 \t Validation acc: 0.77, p: 0.50, r:0.20, f: 0.28, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 4 \t Training Loss: 10.285328\n",
      "Epoch: 4 \t Validation acc: 0.78, p: 0.52, r:0.10, f: 0.17, roc_auc: 0.67, pr_auc: 0.38\n",
      "Epoch: 5 \t Training Loss: 10.284366\n",
      "Epoch: 5 \t Validation acc: 0.77, p: 0.49, r:0.18, f: 0.26, roc_auc: 0.67, pr_auc: 0.38\n",
      "Epoch: 6 \t Training Loss: 10.283634\n",
      "Epoch: 6 \t Validation acc: 0.77, p: 0.49, r:0.16, f: 0.24, roc_auc: 0.67, pr_auc: 0.38\n",
      "2\n",
      "Epoch: 1 \t Training Loss: 10.403895\n",
      "Epoch: 1 \t Validation acc: 0.78, p: 0.53, r:0.11, f: 0.19, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 2 \t Training Loss: 10.288022\n",
      "Epoch: 2 \t Validation acc: 0.78, p: 0.54, r:0.11, f: 0.18, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 3 \t Training Loss: 10.286329\n",
      "Epoch: 3 \t Validation acc: 0.78, p: 0.60, r:0.05, f: 0.10, roc_auc: 0.67, pr_auc: 0.38\n",
      "Epoch: 4 \t Training Loss: 10.285805\n",
      "Epoch: 4 \t Validation acc: 0.77, p: 0.50, r:0.18, f: 0.26, roc_auc: 0.68, pr_auc: 0.38\n",
      "Epoch: 5 \t Training Loss: 10.284697\n",
      "Epoch: 5 \t Validation acc: 0.77, p: 0.49, r:0.20, f: 0.28, roc_auc: 0.67, pr_auc: 0.37\n",
      "Epoch: 6 \t Training Loss: 10.283340\n",
      "Epoch: 6 \t Validation acc: 0.78, p: 0.55, r:0.13, f: 0.21, roc_auc: 0.66, pr_auc: 0.36\n",
      "3\n",
      "Epoch: 1 \t Training Loss: 10.425091\n",
      "Epoch: 1 \t Validation acc: 0.77, p: 0.51, r:0.16, f: 0.25, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 2 \t Training Loss: 10.288169\n",
      "Epoch: 2 \t Validation acc: 0.78, p: 0.53, r:0.12, f: 0.19, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 3 \t Training Loss: 10.286426\n",
      "Epoch: 3 \t Validation acc: 0.77, p: 0.52, r:0.05, f: 0.10, roc_auc: 0.67, pr_auc: 0.38\n",
      "Epoch: 4 \t Training Loss: 10.285444\n",
      "Epoch: 4 \t Validation acc: 0.77, p: 0.48, r:0.20, f: 0.28, roc_auc: 0.68, pr_auc: 0.38\n",
      "Epoch: 5 \t Training Loss: 10.284349\n",
      "Epoch: 5 \t Validation acc: 0.78, p: 0.66, r:0.02, f: 0.03, roc_auc: 0.67, pr_auc: 0.37\n",
      "Epoch: 6 \t Training Loss: 10.283693\n",
      "Epoch: 6 \t Validation acc: 0.77, p: 0.49, r:0.19, f: 0.27, roc_auc: 0.67, pr_auc: 0.38\n",
      "4\n",
      "Epoch: 1 \t Training Loss: 10.392648\n",
      "Epoch: 1 \t Validation acc: 0.78, p: 0.51, r:0.13, f: 0.21, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 2 \t Training Loss: 10.288010\n",
      "Epoch: 2 \t Validation acc: 0.78, p: 0.63, r:0.07, f: 0.12, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 3 \t Training Loss: 10.286384\n",
      "Epoch: 3 \t Validation acc: 0.78, p: 0.56, r:0.09, f: 0.15, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 4 \t Training Loss: 10.285738\n",
      "Epoch: 4 \t Validation acc: 0.78, p: 0.54, r:0.14, f: 0.22, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 5 \t Training Loss: 10.284178\n",
      "Epoch: 5 \t Validation acc: 0.77, p: 0.47, r:0.19, f: 0.27, roc_auc: 0.67, pr_auc: 0.38\n",
      "Epoch: 6 \t Training Loss: 10.284167\n",
      "Epoch: 6 \t Validation acc: 0.77, p: 0.50, r:0.18, f: 0.27, roc_auc: 0.67, pr_auc: 0.38\n",
      "5\n",
      "Epoch: 1 \t Training Loss: 10.436451\n",
      "Epoch: 1 \t Validation acc: 0.78, p: 0.53, r:0.12, f: 0.20, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 2 \t Training Loss: 10.287965\n",
      "Epoch: 2 \t Validation acc: 0.78, p: 0.52, r:0.10, f: 0.17, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 3 \t Training Loss: 10.286206\n",
      "Epoch: 3 \t Validation acc: 0.77, p: 0.49, r:0.08, f: 0.13, roc_auc: 0.67, pr_auc: 0.37\n",
      "Epoch: 4 \t Training Loss: 10.285485\n",
      "Epoch: 4 \t Validation acc: 0.77, p: 0.51, r:0.08, f: 0.14, roc_auc: 0.68, pr_auc: 0.37\n",
      "Epoch: 5 \t Training Loss: 10.284447\n",
      "Epoch: 5 \t Validation acc: 0.77, p: 0.46, r:0.21, f: 0.29, roc_auc: 0.68, pr_auc: 0.37\n",
      "Epoch: 6 \t Training Loss: 10.283924\n",
      "Epoch: 6 \t Validation acc: 0.77, p: 0.41, r:0.03, f: 0.05, roc_auc: 0.66, pr_auc: 0.35\n",
      "6\n",
      "Epoch: 1 \t Training Loss: 10.394755\n",
      "Epoch: 1 \t Validation acc: 0.78, p: 0.53, r:0.13, f: 0.21, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 2 \t Training Loss: 10.288102\n",
      "Epoch: 2 \t Validation acc: 0.78, p: 0.53, r:0.14, f: 0.22, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 3 \t Training Loss: 10.286288\n",
      "Epoch: 3 \t Validation acc: 0.78, p: 0.54, r:0.10, f: 0.17, roc_auc: 0.67, pr_auc: 0.39\n",
      "Epoch: 4 \t Training Loss: 10.285093\n",
      "Epoch: 4 \t Validation acc: 0.77, p: 0.50, r:0.15, f: 0.23, roc_auc: 0.68, pr_auc: 0.38\n",
      "Epoch: 5 \t Training Loss: 10.284857\n",
      "Epoch: 5 \t Validation acc: 0.77, p: 0.48, r:0.22, f: 0.31, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 6 \t Training Loss: 10.284444\n",
      "Epoch: 6 \t Validation acc: 0.78, p: 0.55, r:0.08, f: 0.14, roc_auc: 0.67, pr_auc: 0.39\n",
      "7\n",
      "Epoch: 1 \t Training Loss: 10.412190\n",
      "Epoch: 1 \t Validation acc: 0.78, p: 0.57, r:0.12, f: 0.20, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 2 \t Training Loss: 10.288082\n",
      "Epoch: 2 \t Validation acc: 0.78, p: 0.52, r:0.12, f: 0.19, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 3 \t Training Loss: 10.286663\n",
      "Epoch: 3 \t Validation acc: 0.78, p: 0.51, r:0.16, f: 0.25, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 4 \t Training Loss: 10.285092\n",
      "Epoch: 4 \t Validation acc: 0.77, p: 0.49, r:0.20, f: 0.29, roc_auc: 0.67, pr_auc: 0.37\n",
      "Epoch: 5 \t Training Loss: 10.284693\n",
      "Epoch: 5 \t Validation acc: 0.78, p: 0.53, r:0.07, f: 0.13, roc_auc: 0.66, pr_auc: 0.38\n",
      "Epoch: 6 \t Training Loss: 10.284049\n",
      "Epoch: 6 \t Validation acc: 0.77, p: 0.48, r:0.20, f: 0.28, roc_auc: 0.67, pr_auc: 0.38\n",
      "8\n",
      "Epoch: 1 \t Training Loss: 10.401903\n",
      "Epoch: 1 \t Validation acc: 0.78, p: 0.53, r:0.12, f: 0.19, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 2 \t Training Loss: 10.288459\n",
      "Epoch: 2 \t Validation acc: 0.78, p: 0.52, r:0.12, f: 0.19, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 3 \t Training Loss: 10.286072\n",
      "Epoch: 3 \t Validation acc: 0.78, p: 0.54, r:0.09, f: 0.16, roc_auc: 0.67, pr_auc: 0.39\n",
      "Epoch: 4 \t Training Loss: 10.285357\n",
      "Epoch: 4 \t Validation acc: 0.78, p: 0.52, r:0.18, f: 0.27, roc_auc: 0.67, pr_auc: 0.39\n",
      "Epoch: 5 \t Training Loss: 10.284395\n",
      "Epoch: 5 \t Validation acc: 0.78, p: 0.52, r:0.15, f: 0.23, roc_auc: 0.67, pr_auc: 0.39\n",
      "Epoch: 6 \t Training Loss: 10.283791\n",
      "Epoch: 6 \t Validation acc: 0.78, p: 0.52, r:0.20, f: 0.29, roc_auc: 0.67, pr_auc: 0.39\n",
      "9\n",
      "Epoch: 1 \t Training Loss: 10.393090\n",
      "Epoch: 1 \t Validation acc: 0.78, p: 0.56, r:0.10, f: 0.17, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 2 \t Training Loss: 10.288690\n",
      "Epoch: 2 \t Validation acc: 0.78, p: 0.54, r:0.15, f: 0.24, roc_auc: 0.68, pr_auc: 0.39\n",
      "Epoch: 3 \t Training Loss: 10.286361\n",
      "Epoch: 3 \t Validation acc: 0.78, p: 0.58, r:0.06, f: 0.11, roc_auc: 0.67, pr_auc: 0.38\n",
      "Epoch: 4 \t Training Loss: 10.285619\n",
      "Epoch: 4 \t Validation acc: 0.77, p: 0.50, r:0.20, f: 0.28, roc_auc: 0.68, pr_auc: 0.38\n",
      "Epoch: 5 \t Training Loss: 10.284634\n",
      "Epoch: 5 \t Validation acc: 0.78, p: 0.58, r:0.05, f: 0.10, roc_auc: 0.67, pr_auc: 0.38\n",
      "Epoch: 6 \t Training Loss: 10.284289\n",
      "Epoch: 6 \t Validation acc: 0.77, p: 0.49, r:0.21, f: 0.29, roc_auc: 0.67, pr_auc: 0.37\n"
     ]
    }
   ],
   "source": [
    "results_content = pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_RUNS):\n",
    "    print(i)    \n",
    "    \n",
    "    # CONTENT\n",
    "    content_model = CONTENT(N_VOCAB, EMBED_SIZE, N_HIDDEN, N_TOPICS, MAX_LENGTH)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(content_model.parameters(),lr=LEARNING_RATE)\n",
    "    train_content(content_model, train_loader, val_loader, NUM_EPOCHS, criterion, optimizer)\n",
    "    acc, p, r, f, roc_auc,pr_auc  = eval_model_content(content_model, test_loader)\n",
    "    tmp = pd.DataFrame({'roc_auc':[roc_auc] , 'pr_auc':[pr_auc], 'accuracy':[acc], 'precision':[p], 'recall':[r], 'f1':[f]}).round(2)\n",
    "    results_content = pd.concat([results_content,tmp])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   roc_auc  pr_auc  accuracy  precision  recall    f1\n",
       "0     0.64    0.33      0.77       0.46    0.07  0.11\n",
       "0     0.67    0.37      0.77       0.47    0.15  0.23\n",
       "0     0.66    0.37      0.78       0.52    0.13  0.21\n",
       "0     0.67    0.37      0.77       0.45    0.19  0.26\n",
       "0     0.66    0.37      0.77       0.46    0.18  0.26\n",
       "0     0.65    0.32      0.76       0.30    0.04  0.07\n",
       "0     0.66    0.37      0.78       0.53    0.12  0.19\n",
       "0     0.67    0.36      0.77       0.47    0.20  0.28\n",
       "0     0.65    0.34      0.76       0.42    0.15  0.22\n",
       "0     0.67    0.38      0.77       0.47    0.21  0.29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "roc_auc      0.66\n",
       "pr_auc       0.36\n",
       "accuracy     0.77\n",
       "precision    0.45\n",
       "recall       0.14\n",
       "f1           0.21\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "roc_auc      0.01\n",
       "pr_auc       0.02\n",
       "accuracy     0.01\n",
       "precision    0.06\n",
       "recall       0.06\n",
       "f1           0.07\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results_content)\n",
    "display(results_content.mean().round(2))\n",
    "display(results_content.std().round(2))\n",
    "results_content.to_csv(f\"pytorch_results_content_epoch{NUM_EPOCHS}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run on a Thinkpad machine\n",
    "with 16GB RAM,\n",
    "Processor 11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz, 2803 Mhz, 4 Core(s), 8 Logical Processor(s)\n",
    "Windows 11 OS.\n",
    "Device GPU was not used for training.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}