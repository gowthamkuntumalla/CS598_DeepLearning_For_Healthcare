{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from patient_data_reader import PatientReader\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "\n",
    "class Config:\n",
    "    \"\"\"feel free to play with these hyperparameters during training\"\"\"\n",
    "    dataset = \"resource\"  # change this to the right data name\n",
    "    data_path = \"%s\" % dataset\n",
    "    checkpoint_dir = \"checkpoint\"\n",
    "    decay_rate = 0.95\n",
    "    decay_step = 1000\n",
    "    n_topics = 50\n",
    "    learning_rate = 0.001 # 0.00002\n",
    "    vocab_size = 619\n",
    "    n_stops = 22 \n",
    "    lda_vocab_size = vocab_size - n_stops\n",
    "    n_hidden = 200\n",
    "    n_layers = 2\n",
    "    projector_embed_dim = 100\n",
    "    generator_embed_dim = n_hidden\n",
    "    dropout = 1.0\n",
    "    max_grad_norm = 1.0 #for gradient clipping\n",
    "    total_epoch = 5\n",
    "    init_scale = 0.075\n",
    "    threshold = 0.5 #probability cut-off for predicting label to be 1\n",
    "    forward_only = False #indicates whether we are in testing or training mode\n",
    "    log_dir = 'logs'\n",
    "    \n",
    "    \n",
    "FLAGS = Config()\n",
    "\n",
    "# Define hyperparameters\n",
    "N_HIDDEN = FLAGS.n_hidden\n",
    "NUM_EPOCHS = FLAGS.total_epoch\n",
    "N_VOCAB = FLAGS.vocab_size\n",
    "EMBED_SIZE = FLAGS.projector_embed_dim\n",
    "N_HIDDEN = FLAGS.n_hidden\n",
    "N_TOPICS = FLAGS.n_topics\n",
    "LEARNING_RATE = FLAGS.learning_rate\n",
    "\n",
    "\n",
    "N_BATCH = 128 # 128, 32, 1\n",
    "MAX_LENGTH = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(seqs, labels, vocabsize, maxlen=None):\n",
    "    \"\"\"Create the matrices from the datasets.\n",
    "\n",
    "    This pad each sequence to the same lenght: the lenght of the\n",
    "    longuest sequence or maxlen.\n",
    "\n",
    "    if maxlen is set, we will cut all sequence to this maximum\n",
    "    lenght.\n",
    "\n",
    "    This swap the axis!\n",
    "    \"\"\"\n",
    "    # x: a list of sentences\n",
    "    lengths = [len(s) for s in seqs]\n",
    "\n",
    "    eventSeq = []\n",
    "\n",
    "    for seq in seqs:\n",
    "        t = []\n",
    "        for visit in seq:\n",
    "            t.extend(visit)\n",
    "        eventSeq.append(t)\n",
    "    eventLengths = [len(s) for s in eventSeq]\n",
    "\n",
    "    if maxlen is not None:\n",
    "        new_seqs = []\n",
    "        new_lengths = []\n",
    "        new_labels = []\n",
    "        for l, s, la in zip(lengths, seqs, labels):\n",
    "            if l < maxlen:\n",
    "                new_seqs.append(s)\n",
    "                new_lengths.append(l)\n",
    "                new_labels.append(la)\n",
    "            else:\n",
    "                new_seqs.append(s[:maxlen])\n",
    "                new_lengths.append(maxlen)\n",
    "                new_labels.append(la[:maxlen])\n",
    "        lengths = new_lengths\n",
    "        seqs = new_seqs\n",
    "        labels = new_labels\n",
    "\n",
    "        if len(lengths) < 1:\n",
    "            return None, None, None\n",
    "\n",
    "    n_samples = len(seqs)\n",
    "    maxlen = max(maxlen, np.max(lengths)) # changed this line to always to goto max_len as we use in pytroch with batches\n",
    "\n",
    "    x = np.zeros((n_samples, maxlen, vocabsize)).astype('int64')\n",
    "    x_mask = np.zeros((n_samples, maxlen)).astype(float)\n",
    "    y = np.ones((n_samples, maxlen)).astype(float)\n",
    "    for idx, s in enumerate(seqs):\n",
    "        x_mask[idx, :lengths[idx]] = 1\n",
    "        for j, sj in enumerate(s):\n",
    "            for tsj in sj:\n",
    "                x[idx, j, tsj - 1] = 1\n",
    "    for idx, t in enumerate(labels):\n",
    "        y[idx, :lengths[idx]] = t\n",
    "        # if lengths[idx] < maxlen:\n",
    "        #     y[idx,lengths[idx]:] = t[-1]\n",
    "    \n",
    "#     # randomly generated list of labels. for testing. note that this size is n_samples,1 and not n_samples,n_visits\n",
    "#     y = torch.randint(0, 2, (n_samples,)) #.astype(float)\n",
    "    return x, x_mask, y, lengths, eventLengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] load resource\\vocab.pkl\n",
      " [*] load resource/X_train.pkl\n",
      " [*] load resource/Y_train.pkl\n",
      " [*] load resource/X_valid.pkl\n",
      " [*] load resource/Y_valid.pkl\n",
      " [*] load resource/X_test.pkl\n",
      " [*] load resource/Y_test.pkl\n",
      "vocabulary size: 619\n",
      "number of training documents: 2000\n",
      "number of validation documents: 500\n",
      "number of testing documents: 500\n",
      "legth of dataset of dtype = train: 2000\n",
      "legth of dataset of dtype = valid: 500\n",
      "legth of dataset of dtype = test: 500\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, seqs, hfs):\n",
    "        self.x = seqs\n",
    "        self.y = hfs\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    \n",
    "\n",
    "data_sets = PatientReader(FLAGS)\n",
    "\n",
    "def get_custom_dataset(dtype):\n",
    "    \"\"\" dtype in train, valid, test\"\"\"\n",
    "    X_raw_data, Y_raw_data = data_sets.get_data_from_type(dtype)\n",
    "    dataset = CustomDataset(X_raw_data, Y_raw_data)\n",
    "    print(f\"legth of dataset of dtype = {dtype}:\", len(X_raw_data))\n",
    "    return dataset\n",
    "\n",
    "train_dataset = get_custom_dataset(\"train\")\n",
    "val_dataset = get_custom_dataset(\"valid\")\n",
    "test_dataset =get_custom_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "     Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "\n",
    "    x, x_mask, y, lengths, eventLengths = prepare_data(seqs=sequences, labels=labels, vocabsize=N_VOCAB, maxlen=MAX_LENGTH)\n",
    "    \n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    x_mask = torch.tensor(x_mask, dtype=torch.bool)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "    return x, x_mask, y, lengths, eventLengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data.dataset import random_split\n",
    "\n",
    "# split = int(len(dataset)*0.5)\n",
    "\n",
    "# lengths = [split, len(dataset) - split]\n",
    "# train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "# print(\"Length of train dataset:\", len(train_dataset))\n",
    "# print(\"Length of val dataset:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset,test_dataset, collate_fn,batch_size=128):\n",
    "    \n",
    "    '''\n",
    "    Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to batch_size. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        test dataset: test dataset of type `CustomDataset`\n",
    "        \n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader, test_dataset : train and validation and test dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset,test_dataset, collate_fn,batch_size = N_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "#     print(hidden_states.shape)  # torch.Size([32, 175, 256])\n",
    "\n",
    "#     print(masks.shape) #torch.Size([32, 175])\n",
    "#     \"\"\"\n",
    "#      obtain the hidden state for the last true visit (not padding visits)\n",
    "\n",
    "#     Arguments:\n",
    "#         hidden_states: the hidden states of each visit of shape (batch_size, # visits, embedding_dim)\n",
    "#         masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "#     Outputs:\n",
    "#         last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim)\n",
    "        \n",
    "#     NOTE: DO NOT use for loop.\n",
    "    \n",
    "#     HINT: First convert the mask to a vector of shape (batch_size,) containing the true visit length; \n",
    "#           and then use this length vector as index to select the last visit.\n",
    "#     \"\"\"\n",
    "\n",
    "    mask_length = masks.count_nonzero(dim=1)\n",
    "    return hidden_states[range(hidden_states.shape[0]),mask_length-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# input = torch.randn(batch_size, sequence_length, input_size)\n",
    "\n",
    "print_flag = False\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUModel, self).__init__()\n",
    "#         l_embed = lasagne.layers.DenseLayer(l_in, num_units=embedsize, b=None, num_leading_axes=2)\n",
    "        self.embed = nn.Linear(N_VOCAB, EMBED_SIZE, bias=False)\n",
    "        self.gru = nn.GRU(input_size=EMBED_SIZE, hidden_size=N_HIDDEN, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features= N_HIDDEN, out_features=MAX_LENGTH)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, masks):\n",
    "        if print_flag: print( \"x\" ,x.shape)\n",
    "        if print_flag: print( \"masks\",masks.shape)\n",
    "        batch_size = x.shape[0]\n",
    "        if print_flag: print( \"batch_size\", batch_size)\n",
    "        x_embed = self.embed(x)\n",
    "        if print_flag: print( \"x_embed\", x_embed.shape)\n",
    "        output, h_n = self.gru(x_embed)\n",
    "        if print_flag: print( \"output\", output.shape)\n",
    "        if print_flag: print( \"h_n\", h_n.shape)\n",
    "        true_h_n = get_last_visit(output, masks)\n",
    "        if print_flag: print( \"true_h_n\",true_h_n.shape)\n",
    "        logits = self.fc(true_h_n)   \n",
    "        if print_flag: print( \"logits\",logits.shape)\n",
    "        probs = self.sigmoid(logits)\n",
    "        if print_flag: print( \"probs\",probs.shape)\n",
    "        probs_ret = probs.view((batch_size,-1))\n",
    "        if print_flag: print( \"probs_ret\",probs_ret.shape)\n",
    "        return probs_ret\n",
    "    \n",
    "## H0 defaults to zeros if not provided.\n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(1, N_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUModel(\n",
       "  (embed): Linear(in_features=619, out_features=100, bias=False)\n",
       "  (gru): GRU(100, 200, batch_first=True)\n",
       "  (fc): Linear(in_features=200, out_features=300, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_rnn = GRUModel()\n",
    "gru_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 300, 619]), torch.Size([128, 300]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "x, x_mask, y, lengths, eventLengths = next(train_iter)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(gru_rnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score #,pr_auc\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "        \n",
    "    HINT: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    def apply_mask(lengths, y):\n",
    "        \"\"\"\n",
    "        for metrics need to somehow for each patient get only real n_visit not all of max_length.\n",
    "            like new_testlabels.extend(inputs[1].flatten()[:leng])\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for i, l in enumerate(lengths):\n",
    "            result.extend(y[i][:l])\n",
    "        return result\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = [] # torch.LongTensor()\n",
    "    y_score = []#torch.Tensor()\n",
    "    y_true = []#torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, x_mask, y, lengths, eventLengths in data_loader:\n",
    "#         print(\"lengths: \", lengths)\n",
    "        y_hat_prob = model(x, x_mask)\n",
    "        y_score.extend(apply_mask(lengths, y_hat_prob.detach().to(device)))\n",
    "        \n",
    "        y_hat = (y_hat_prob > 0.5).int()\n",
    "#         print(np.shape(y_pred), np.shape(y_true))\n",
    "        y_pred.extend(apply_mask(lengths, y_hat.detach().to(device)))\n",
    "        y_true.extend(apply_mask(lengths, y.detach().to(device)))\n",
    "#         print(np.shape(y_pred), np.shape(y_true))\n",
    "#         print(y_pred[:2])\n",
    "        \n",
    "    \"\"\"\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "    \n",
    "#     print(\"after loop: \", np.shape(y_pred), np.shape(y_true))\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "#     pr_auc = pr_auc(new_testlabels, pred_testlabels)\n",
    "    return p, r, f, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        \n",
    "    You need to call `eval_model()` at the end of each training epoch to see how well the model performs \n",
    "    on validation data.\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, x_mask, y, lengths, eventLengths in train_loader:\n",
    "            \"\"\"\n",
    "                1. zero grad\n",
    "                2. model forward\n",
    "                3. calculate loss\n",
    "                4. loss backward\n",
    "                5. optimizer step\n",
    "            \"\"\"\n",
    "            outputs = model(x, x_mask)\n",
    "#             print(\"outputs\",outputs.shape)\n",
    "#             print(\"y\",y.shape)\n",
    "            loss = criterion(outputs, y) \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "              .format(epoch+1, p, r, f, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 0.557629\n",
      "Epoch: 1 \t Validation p: 0.23, r:0.32, f: 0.27, roc_auc: 0.50\n",
      "Epoch: 2 \t Training Loss: 0.286657\n",
      "Epoch: 2 \t Validation p: 0.23, r:0.34, f: 0.27, roc_auc: 0.50\n",
      "Epoch: 3 \t Training Loss: 0.273818\n",
      "Epoch: 3 \t Validation p: 0.23, r:0.32, f: 0.27, roc_auc: 0.50\n",
      "Epoch: 4 \t Training Loss: 0.271648\n",
      "Epoch: 4 \t Validation p: 0.23, r:0.33, f: 0.27, roc_auc: 0.50\n",
      "Epoch: 5 \t Training Loss: 0.270644\n",
      "Epoch: 5 \t Validation p: 0.23, r:0.33, f: 0.27, roc_auc: 0.50\n"
     ]
    }
   ],
   "source": [
    "train(gru_rnn, train_loader, val_loader, NUM_EPOCHS, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation roc_auc:  0.49878489869291753\n",
      "test roc_auc:  0.5223959817316578\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc = eval_model(gru_rnn, val_loader)\n",
    "print(\"validation roc_auc: \", roc_auc)\n",
    "# assert roc_auc > 0.7, \"ROC AUC is too low on the validation set (%f < 0.7)\"%(roc_auc)\n",
    "\n",
    "\n",
    "p, r, f, roc_auc = eval_model(gru_rnn, test_loader)\n",
    "print(\"test roc_auc: \", roc_auc)\n",
    "# assert roc_auc > 0.7, \"ROC AUC is too low on the test set (%f < 0.7)\"%(roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CONTENT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([128, 300, 619])\n",
      "x_emb torch.Size([128, 300, 100])\n",
      "mask torch.Size([128, 300])\n",
      "x_forward torch.Size([128, 300, 200])\n",
      "mu torch.Size([128, 300, 50])\n",
      "log_sigma torch.Size([128, 300, 50])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (200) must match the size of tensor b (50) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 133\u001b[0m\n\u001b[0;32m    131\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[0;32m    132\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(content_model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n\u001b[1;32m--> 133\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, n_epochs, criterion, optimizer)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x, x_mask, y, lengths, eventLengths \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m                1. zero grad\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m                2. model forward\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m                5. optimizer step\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m            \"\"\"\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#             print(\"outputs\",outputs.shape)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#             print(\"y\",y.shape)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m             loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y) \n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[38], line 97\u001b[0m, in \u001b[0;36mCONTENT.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_sigma\u001b[39m\u001b[38;5;124m\"\u001b[39m,log_sigma\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m#         theta = self.l_theta(mu, log_sigma)\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m         theta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml_theta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheta\u001b[39m\u001b[38;5;124m\"\u001b[39m,theta\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    102\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml_B(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[38], line 38\u001b[0m, in \u001b[0;36mThetaLayer.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     36\u001b[0m klterm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(logsigma_in, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m (mu_in \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m (torch\u001b[38;5;241m.\u001b[39mexp(logsigma_in) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mklterm \u001b[38;5;241m=\u001b[39m klterm\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m---> 38\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogsigma_in\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;241m-\u001b[39m((torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;241m-\u001b[39m mu_in) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (torch\u001b[38;5;241m.\u001b[39mexp(logsigma_in) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)))\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(out\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (200) must match the size of tensor b (50) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class ThetaLayer(nn.Module):\n",
    "#     def __init__(self, n_topics, max_length):\n",
    "#         super(ThetaLayer, self).__init__()\n",
    "#         self.n_topics = n_topics\n",
    "#         self.max_length = max_length\n",
    "#         self.klterm = None\n",
    "\n",
    "#     def forward(self, mu, log_sigma):\n",
    "#         self.klterm = -0.5 * torch.sum(1 + 2 * log_sigma - mu.pow(2) - torch.exp(log_sigma).pow(2)) / mu.size(0)\n",
    "#         eps = torch.randn(mu.size(0), self.max_length, self.n_topics).to(mu.device)\n",
    "#         theta = mu.unsqueeze(1) + torch.exp(log_sigma).unsqueeze(1) * eps\n",
    "#         self.theta = nn.Parameter(theta.mean(dim=0, keepdim=False))\n",
    "#         return theta\n",
    "\n",
    "\n",
    "\n",
    "class ThetaLayer(nn.Module):\n",
    "    def __init__(self, incomings, maxlen, **kwargs):\n",
    "        super().__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.logsigma = incomings[1]\n",
    "        self.mu = incomings[0]\n",
    "        self.theta = nn.Parameter(torch.zeros((1, self.logsigma.out_features)))\n",
    "        self.register_buffer('klterm', torch.zeros((1, self.logsigma.out_features)))\n",
    "\n",
    "    def forward(self, input):\n",
    "#         logsigma_in = self.logsigma(input[0])\n",
    "#         mu_in = self.mu(input[1])\n",
    "        logsigma_in = self.logsigma(input)\n",
    "        mu_in = self.mu(input)\n",
    "        klterm = 0.5 * (1 + torch.mul(logsigma_in, 2) - (mu_in ** 2) - (torch.exp(logsigma_in) ** 2))\n",
    "        self.klterm = klterm.detach()\n",
    "        out = 1 / (input * (torch.exp(logsigma_in) * (2 * np.pi) ** (1 / 2))) * torch.exp(\n",
    "            -((torch.log(input) - mu_in) ** 2) / (2 * (torch.exp(logsigma_in) ** 2)))\n",
    "        self.theta = nn.Parameter(out.mean(dim=0, keepdim=True))\n",
    "        return out\n",
    "    \n",
    "        # what is input?????????  \n",
    "class CONTENT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, n_hidden, n_topics, maxlen):\n",
    "        super(CONTENT, self).__init__()\n",
    "        \n",
    "        # embed layer to reduce dimensionality\n",
    "        self.l_embed = nn.Linear(vocab_size, embed_size, bias=False)\n",
    "        \n",
    "        # GRU\n",
    "        self.l_forward0 = nn.GRU(embed_size, n_hidden, batch_first=True, bidirectional=False)\n",
    "\n",
    "        # Recognition Net\n",
    "        self.l_1 = nn.Linear(vocab_size, n_hidden)\n",
    "        self.l_2 = nn.Linear(n_hidden, n_hidden)\n",
    "        \n",
    "        self.mu = nn.Linear(n_hidden, n_topics)\n",
    "        self.log_sigma = nn.Linear(n_hidden, n_topics)\n",
    "#         self.l_theta = ThetaLayer(n_topics, maxlen)\n",
    "        self.l_theta = ThetaLayer([self.mu, self.log_sigma], maxlen)\n",
    "\n",
    "        \n",
    "        self.l_B = nn.Linear(vocab_size, n_topics, bias=False) \n",
    "        self.l_context = nn.Sequential(\n",
    "            nn.BatchNorm1d(n_topics),\n",
    "            nn.Linear(n_topics, n_topics)\n",
    "        )\n",
    "        self.l_dense0 = nn.Linear(n_hidden, 1)\n",
    "        self.l_dense1 = nn.Flatten(start_dim=1)\n",
    "        self.l_dense = nn.Sequential(\n",
    "            nn.Linear(n_topics + maxlen, 1),\n",
    "            nn.Flatten(start_dim=1),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        print(\"x\", x.shape)\n",
    "        \n",
    "        x_emb = self.l_embed(x)\n",
    "        print(\"x_emb\",x_emb.shape)\n",
    "        \n",
    "        x_forward, _ = self.l_forward0(x_emb)\n",
    "        print(\"mask\",mask.shape)\n",
    "        \n",
    "        x_forward = x_forward * mask.unsqueeze(2)\n",
    "        print(\"x_forward\",x_forward.shape)\n",
    "\n",
    "        x_1 = F.relu(self.l_1(x))\n",
    "        x_2 = F.relu(self.l_2(x_1))\n",
    "        mu = self.mu(x_2)\n",
    "        log_sigma = self.log_sigma(x_2)\n",
    "        print(\"mu\",mu.shape)\n",
    "        print(\"log_sigma\",log_sigma.shape)\n",
    "        \n",
    "#         theta = self.l_theta(mu, log_sigma)\n",
    "        theta = self.l_theta(x_2)\n",
    "\n",
    "        print(\"theta\",theta.shape)\n",
    "        \n",
    "\n",
    "        b = self.l_B(x)\n",
    "        print(\"b\",b.shape)\n",
    "        \n",
    "        context = b * theta\n",
    "        print(\"context before mean\",context.shape)\n",
    "        context = torch.mean(context, dim=-1)\n",
    "#         context = b * theta\n",
    "        print(\"context\",context.shape)\n",
    "        \n",
    "        # combine GRU and context\n",
    "\n",
    "        x_dense0 = self.l_dense0(x_forward)\n",
    "        print(\"x_dense0\",x_dense0.shape)\n",
    "        \n",
    "        x_dense1 = self.l_dense1(x_dense0)\n",
    "        print(\"x_dense1\",x_dense1.shape)\n",
    "        x_dense = self.l_dense(torch.cat((x_dense1, context), dim=1))\n",
    "        print(\"x_dense\",x_dense.shape)\n",
    "\n",
    "        output = torch.sigmoid(x_dense) * mask + 0.000001\n",
    "        print(\"output\",output.shape)\n",
    "        out = output.view(-1, maxlen)\n",
    "        print(\"out\",out.shape)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "# Define model and optimizer\n",
    "content_model = CONTENT(N_VOCAB, EMBED_SIZE, N_HIDDEN, N_TOPICS, MAX_LENGTH)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(content_model.parameters(),lr=LEARNING_RATE)\n",
    "train(content_model, train_loader, val_loader, NUM_EPOCHS, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)\n",
    "x, x_mask, y, lengths, eventLengths = next(train_iter)\n",
    "x.shape, y.shape\n",
    "\n",
    "#3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: need to write cost function which include klterm as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, accuracy_score\n",
    "# import time\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # define the training function\n",
    "# def train(model, optimizer, criterion, x, y, mask):\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(x, mask)\n",
    "#     loss = criterion(output.flatten(), y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     return loss.item()\n",
    "\n",
    "# # define the function to compute cost\n",
    "# def compute_cost(model, x, y, mask):\n",
    "#     output = model(x, mask)\n",
    "#     loss = nn.BCELoss()(output.flatten(), y)\n",
    "#     return loss.item()\n",
    "\n",
    "# # define the function to output the hidden state\n",
    "# def output_theta(model, x, mask):\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "#         output, hn = model.rnn(x)\n",
    "#         output = output * mask.unsqueeze(-1)\n",
    "#         output = output.sum(dim=1) / mask.sum(dim=1).unsqueeze(-1)\n",
    "#     return hn.detach().numpy().reshape(x.shape[0], -1), output.detach().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
