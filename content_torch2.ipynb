{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from patient_data_reader import PatientReader\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "\n",
    "class Config:\n",
    "    \"\"\"feel free to play with these hyperparameters during training\"\"\"\n",
    "    dataset = \"resource\"  # change this to the right data name\n",
    "    data_path = \"%s\" % dataset\n",
    "    checkpoint_dir = \"checkpoint\"\n",
    "    decay_rate = 0.95\n",
    "    decay_step = 1000\n",
    "    n_topics = 50\n",
    "    learning_rate = 0.001 # 0.00002\n",
    "    vocab_size = 619\n",
    "    n_stops = 22 \n",
    "    lda_vocab_size = vocab_size - n_stops\n",
    "    n_hidden = 200\n",
    "    n_layers = 2\n",
    "    projector_embed_dim = 100\n",
    "    generator_embed_dim = n_hidden\n",
    "    dropout = 1.0\n",
    "    max_grad_norm = 1.0 #for gradient clipping\n",
    "    total_epoch = 5\n",
    "    init_scale = 0.075\n",
    "    threshold = 0.5 #probability cut-off for predicting label to be 1\n",
    "    forward_only = False #indicates whether we are in testing or training mode\n",
    "    log_dir = 'logs'\n",
    "    \n",
    "    \n",
    "FLAGS = Config()\n",
    "\n",
    "# Define hyperparameters\n",
    "N_HIDDEN = FLAGS.n_hidden\n",
    "NUM_EPOCHS = FLAGS.total_epoch\n",
    "N_VOCAB = FLAGS.vocab_size\n",
    "EMBED_SIZE = FLAGS.projector_embed_dim\n",
    "N_HIDDEN = FLAGS.n_hidden\n",
    "N_TOPICS = FLAGS.n_topics\n",
    "LEARNING_RATE = FLAGS.learning_rate\n",
    "\n",
    "\n",
    "N_BATCH = 128 # 128, 32, 1\n",
    "MAX_LENGTH = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(seqs, labels, vocabsize, maxlen=None):\n",
    "    \"\"\"Create the matrices from the datasets.\n",
    "\n",
    "    This pad each sequence to the same lenght: the lenght of the\n",
    "    longuest sequence or maxlen.\n",
    "\n",
    "    if maxlen is set, we will cut all sequence to this maximum\n",
    "    lenght.\n",
    "\n",
    "    This swap the axis!\n",
    "    \"\"\"\n",
    "    # x: a list of sentences\n",
    "    lengths = [len(s) for s in seqs]\n",
    "\n",
    "    eventSeq = []\n",
    "\n",
    "    for seq in seqs:\n",
    "        t = []\n",
    "        for visit in seq:\n",
    "            t.extend(visit)\n",
    "        eventSeq.append(t)\n",
    "    eventLengths = [len(s) for s in eventSeq]\n",
    "\n",
    "    if maxlen is not None:\n",
    "        new_seqs = []\n",
    "        new_lengths = []\n",
    "        new_labels = []\n",
    "        for l, s, la in zip(lengths, seqs, labels):\n",
    "            if l < maxlen:\n",
    "                new_seqs.append(s)\n",
    "                new_lengths.append(l)\n",
    "                new_labels.append(la)\n",
    "            else:\n",
    "                new_seqs.append(s[:maxlen])\n",
    "                new_lengths.append(maxlen)\n",
    "                new_labels.append(la[:maxlen])\n",
    "        lengths = new_lengths\n",
    "        seqs = new_seqs\n",
    "        labels = new_labels\n",
    "\n",
    "        if len(lengths) < 1:\n",
    "            return None, None, None\n",
    "\n",
    "    n_samples = len(seqs)\n",
    "    maxlen = max(maxlen, np.max(lengths)) # changed this line to always to goto max_len as we use in pytroch with batches\n",
    "\n",
    "    x = np.zeros((n_samples, maxlen, vocabsize)).astype('int64')\n",
    "    x_mask = np.zeros((n_samples, maxlen)).astype(float)\n",
    "    y = np.ones((n_samples, maxlen)).astype(float)\n",
    "    for idx, s in enumerate(seqs):\n",
    "        x_mask[idx, :lengths[idx]] = 1\n",
    "        for j, sj in enumerate(s):\n",
    "            for tsj in sj:\n",
    "                x[idx, j, tsj - 1] = 1\n",
    "    for idx, t in enumerate(labels):\n",
    "        y[idx, :lengths[idx]] = t\n",
    "        # if lengths[idx] < maxlen:\n",
    "        #     y[idx,lengths[idx]:] = t[-1]\n",
    "    \n",
    "#     # randomly generated list of labels. for testing. note that this size is n_samples,1 and not n_samples,n_visits\n",
    "#     y = torch.randint(0, 2, (n_samples,)) #.astype(float)\n",
    "    return x, x_mask, y, lengths, eventLengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] load resource\\vocab.pkl\n",
      " [*] load resource/X_train.pkl\n",
      " [*] load resource/Y_train.pkl\n",
      " [*] load resource/X_valid.pkl\n",
      " [*] load resource/Y_valid.pkl\n",
      " [*] load resource/X_test.pkl\n",
      " [*] load resource/Y_test.pkl\n",
      "vocabulary size: 619\n",
      "number of training documents: 2000\n",
      "number of validation documents: 500\n",
      "number of testing documents: 500\n",
      "legth of dataset of dtype = train: 2000\n",
      "legth of dataset of dtype = valid: 500\n",
      "legth of dataset of dtype = test: 500\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, seqs, hfs):\n",
    "        self.x = seqs\n",
    "        self.y = hfs\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    \n",
    "\n",
    "data_sets = PatientReader(FLAGS)\n",
    "\n",
    "def get_custom_dataset(dtype):\n",
    "    \"\"\" dtype in train, valid, test\"\"\"\n",
    "    X_raw_data, Y_raw_data = data_sets.get_data_from_type(dtype)\n",
    "    dataset = CustomDataset(X_raw_data, Y_raw_data)\n",
    "    print(f\"legth of dataset of dtype = {dtype}:\", len(X_raw_data))\n",
    "    return dataset\n",
    "\n",
    "train_dataset = get_custom_dataset(\"train\")\n",
    "val_dataset = get_custom_dataset(\"valid\")\n",
    "test_dataset =get_custom_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "     Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "\n",
    "    x, x_mask, y, lengths, eventLengths = prepare_data(seqs=sequences, labels=labels, vocabsize=N_VOCAB, maxlen=MAX_LENGTH)\n",
    "    \n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    x_mask = torch.tensor(x_mask, dtype=torch.bool)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "    return x, x_mask, y, lengths, eventLengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data.dataset import random_split\n",
    "\n",
    "# split = int(len(dataset)*0.5)\n",
    "\n",
    "# lengths = [split, len(dataset) - split]\n",
    "# train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "# print(\"Length of train dataset:\", len(train_dataset))\n",
    "# print(\"Length of val dataset:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset,test_dataset, collate_fn,batch_size=128):\n",
    "    \n",
    "    '''\n",
    "    Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to batch_size. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        test dataset: test dataset of type `CustomDataset`\n",
    "        \n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader, test_dataset : train and validation and test dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset,test_dataset, collate_fn,batch_size = N_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "#     print(hidden_states.shape)  # torch.Size([32, 175, 256])\n",
    "\n",
    "#     print(masks.shape) #torch.Size([32, 175])\n",
    "#     \"\"\"\n",
    "#      obtain the hidden state for the last true visit (not padding visits)\n",
    "\n",
    "#     Arguments:\n",
    "#         hidden_states: the hidden states of each visit of shape (batch_size, # visits, embedding_dim)\n",
    "#         masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "#     Outputs:\n",
    "#         last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim)\n",
    "        \n",
    "#     NOTE: DO NOT use for loop.\n",
    "    \n",
    "#     HINT: First convert the mask to a vector of shape (batch_size,) containing the true visit length; \n",
    "#           and then use this length vector as index to select the last visit.\n",
    "#     \"\"\"\n",
    "\n",
    "    mask_length = masks.count_nonzero(dim=1)\n",
    "    return hidden_states[range(hidden_states.shape[0]),mask_length-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# input = torch.randn(batch_size, sequence_length, input_size)\n",
    "\n",
    "print_flag = False\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUModel, self).__init__()\n",
    "#         l_embed = lasagne.layers.DenseLayer(l_in, num_units=embedsize, b=None, num_leading_axes=2)\n",
    "        self.embed = nn.Linear(N_VOCAB, EMBED_SIZE, bias=False)\n",
    "        self.gru = nn.GRU(input_size=EMBED_SIZE, hidden_size=N_HIDDEN, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features= N_HIDDEN, out_features=MAX_LENGTH)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, masks):\n",
    "        if print_flag: print( \"x\" ,x.shape)\n",
    "        if print_flag: print( \"masks\",masks.shape)\n",
    "        batch_size = x.shape[0]\n",
    "        if print_flag: print( \"batch_size\", batch_size)\n",
    "        x_embed = self.embed(x)\n",
    "        if print_flag: print( \"x_embed\", x_embed.shape)\n",
    "        output, h_n = self.gru(x_embed)\n",
    "        if print_flag: print( \"output\", output.shape)\n",
    "        if print_flag: print( \"h_n\", h_n.shape)\n",
    "        true_h_n = get_last_visit(output, masks)\n",
    "        if print_flag: print( \"true_h_n\",true_h_n.shape)\n",
    "        logits = self.fc(true_h_n)   \n",
    "        if print_flag: print( \"logits\",logits.shape)\n",
    "        probs = self.sigmoid(logits)\n",
    "        if print_flag: print( \"probs\",probs.shape)\n",
    "        probs_ret = probs.view((batch_size,-1))\n",
    "        if print_flag: print( \"probs_ret\",probs_ret.shape)\n",
    "        return probs_ret\n",
    "    \n",
    "## H0 defaults to zeros if not provided.\n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(1, N_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUModel(\n",
       "  (embed): Linear(in_features=619, out_features=100, bias=False)\n",
       "  (gru): GRU(100, 200, batch_first=True)\n",
       "  (fc): Linear(in_features=200, out_features=300, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_rnn = GRUModel()\n",
    "gru_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 300, 619]), torch.Size([128, 300]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "x, x_mask, y, lengths, eventLengths = next(train_iter)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(gru_rnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score #,pr_auc\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "        \n",
    "    HINT: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    def apply_mask(lengths, y):\n",
    "        \"\"\"\n",
    "        for metrics need to somehow for each patient get only real n_visit not all of max_length.\n",
    "            like new_testlabels.extend(inputs[1].flatten()[:leng])\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for i, l in enumerate(lengths):\n",
    "            result.extend(y[i][:l])\n",
    "        return result\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = [] # torch.LongTensor()\n",
    "    y_score = []#torch.Tensor()\n",
    "    y_true = []#torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, x_mask, y, lengths, eventLengths in data_loader:\n",
    "#         print(\"lengths: \", lengths)\n",
    "        y_hat_prob = model(x, x_mask)\n",
    "        y_score.extend(apply_mask(lengths, y_hat_prob.detach().to(device)))\n",
    "        \n",
    "        y_hat = (y_hat_prob > 0.5).int()\n",
    "#         print(np.shape(y_pred), np.shape(y_true))\n",
    "        y_pred.extend(apply_mask(lengths, y_hat.detach().to(device)))\n",
    "        y_true.extend(apply_mask(lengths, y.detach().to(device)))\n",
    "#         print(np.shape(y_pred), np.shape(y_true))\n",
    "#         print(y_pred[:2])\n",
    "        \n",
    "    \"\"\"\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "    \n",
    "#     print(\"after loop: \", np.shape(y_pred), np.shape(y_true))\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "#     pr_auc = pr_auc(new_testlabels, pred_testlabels)\n",
    "    return p, r, f, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        \n",
    "    You need to call `eval_model()` at the end of each training epoch to see how well the model performs \n",
    "    on validation data.\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, x_mask, y, lengths, eventLengths in train_loader:\n",
    "            \"\"\"\n",
    "                1. zero grad\n",
    "                2. model forward\n",
    "                3. calculate loss\n",
    "                4. loss backward\n",
    "                5. optimizer step\n",
    "            \"\"\"\n",
    "            outputs = model(x, x_mask)\n",
    "#             print(\"outputs\",outputs.shape)\n",
    "#             print(\"y\",y.shape)\n",
    "            loss = criterion(outputs, y) \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "              .format(epoch+1, p, r, f, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 0.567484\n",
      "Epoch: 1 \t Validation p: 0.23, r:0.33, f: 0.27, roc_auc: 0.50\n",
      "Epoch: 2 \t Training Loss: 0.288260\n",
      "Epoch: 2 \t Validation p: 0.23, r:0.32, f: 0.27, roc_auc: 0.50\n",
      "Epoch: 3 \t Training Loss: 0.273747\n",
      "Epoch: 3 \t Validation p: 0.23, r:0.33, f: 0.27, roc_auc: 0.50\n",
      "Epoch: 4 \t Training Loss: 0.271507\n",
      "Epoch: 4 \t Validation p: 0.23, r:0.32, f: 0.27, roc_auc: 0.50\n",
      "Epoch: 5 \t Training Loss: 0.270339\n",
      "Epoch: 5 \t Validation p: 0.23, r:0.32, f: 0.27, roc_auc: 0.50\n"
     ]
    }
   ],
   "source": [
    "train(gru_rnn, train_loader, val_loader, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation roc_auc:  0.4987337207345152\n",
      "test roc_auc:  0.523144024013876\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc = eval_model(gru_rnn, val_loader)\n",
    "print(\"validation roc_auc: \", roc_auc)\n",
    "# assert roc_auc > 0.7, \"ROC AUC is too low on the validation set (%f < 0.7)\"%(roc_auc)\n",
    "\n",
    "\n",
    "p, r, f, roc_auc = eval_model(gru_rnn, test_loader)\n",
    "print(\"test roc_auc: \", roc_auc)\n",
    "# assert roc_auc > 0.7, \"ROC AUC is too low on the test set (%f < 0.7)\"%(roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CONTENT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class ExpressionLayer(nn.Module):\n",
    "#     def __init__(self, expression, output_shape):\n",
    "#         super().__init__()\n",
    "#         self.expression = expression\n",
    "#         self.output_shape_ = output_shape\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.expression(x)\n",
    "\n",
    "#     def get_output_shape_for(self, input_shape):\n",
    "#         return (input_shape[0],) + self.output_shape_\n",
    "\n",
    "class ThetaLayer(nn.Module):\n",
    "    def __init__(self, mu_layer, logsigma_layer, maxlen):\n",
    "        super().__init__()\n",
    "        self.mu = mu_layer\n",
    "        self.logsigma = logsigma_layer\n",
    "        self.klterm = 0\n",
    "        self.theta = nn.Parameter(torch.zeros(1, logsigma_layer.out_features))\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def forward(self, x):\n",
    "        logsigma_in = self.logsigma(x[1])\n",
    "        mu_in = self.mu(x[0])\n",
    "        kltermFn = lambda logsigma, mu: 0.5 * (1 + logsigma * 2 - (mu ** 2) - torch.exp(logsigma) ** 2)\n",
    "        self.klterm = (0.5 * (1 + logsigma_in * 2) - (mu_in ** 2) - (torch.exp(logsigma_in) ** 2))\n",
    "        out = lambda mu, logsigma, input: (1 / (input * (torch.exp(logsigma) * (2 * np.pi) ** (1 / 2)))) * torch.exp(\n",
    "            -((torch.log(input) - mu) ** 2) / (2 * (torch.exp(logsigma) ** 2)))\n",
    "        self.theta = nn.Parameter(\n",
    "            out(mu_in, logsigma_in, x).reshape(x[0].size(0), self.maxlen, self.logsigma.out_features))\n",
    "        return self.theta\n",
    "\n",
    "# class ThetaLayer(nn.Module):\n",
    "#     def __init__(self, mu, log_sigma, maxlen=MAX_LENGTH):\n",
    "#         super(ThetaLayer, self).__init__()\n",
    "#         self.mu = mu\n",
    "#         self.log_sigma = log_sigma\n",
    "#         self.maxlen = maxlen\n",
    "#         self.klterm = None\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         self.mu = F.dropout(self.mu, p=0.5, training=self.training)\n",
    "#         self.log_sigma = F.dropout(self.log_sigma, p=0.5, training=self.training)\n",
    "#         epsilon = torch.randn(self.mu.size(), device=x.device)\n",
    "#         z = self.mu + torch.exp(self.log_sigma / 2) * epsilon\n",
    "#         self.klterm = -0.5 * torch.mean(\n",
    "#             torch.sum(1 + 2 * self.log_sigma - self.mu.pow(2) - torch.exp(2 * self.log_sigma), dim=2), dim=1)\n",
    "#         theta = F.pad(z, (0, self.maxlen - z.size()[1], 0, 0))\n",
    "#         return theta\n",
    "\n",
    "class CONTENT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, n_hidden, n_topics, maxlen):\n",
    "        super(CONTENT, self).__init__()\n",
    "        self.embed = nn.Linear(vocab_size, embed_size, bias=False)\n",
    "        \n",
    "        # GRU\n",
    "        self.gru = nn.GRU(embed_size, n_hidden, bidirectional=False, batch_first=True)\n",
    "        \n",
    "        # RECOGNET\n",
    "        self.dense1 = nn.Linear(vocab_size, n_hidden)\n",
    "        self.dense2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.mu = nn.Linear(n_hidden, n_topics)\n",
    "        self.log_sigma = nn.Linear(n_hidden, n_topics)\n",
    "        self.theta = ThetaLayer(self.mu, self.log_sigma, maxlen) #ThetaLayer(None, None, maxlen)\n",
    "        self.B = nn.Linear(vocab_size, n_topics, bias=False)\n",
    "        self.context = nn.Sequential(nn.Linear(n_topics, 1, bias=False), nn.Flatten(1))\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x_embed = self.embed(x)\n",
    "        \n",
    "        # GRU forward\n",
    "        h, _ = self.gru(x_embed)\n",
    "        h_masked = h * mask[:, :, None]\n",
    "        h_masked = h_masked.reshape((-1, h_masked.size()[2]))\n",
    "        \n",
    "        # recog net forward\n",
    "        h1 = F.relu(self.dense1(x))\n",
    "        h2 = F.relu(self.dense2(h1))\n",
    "        mu = self.mu(h2)\n",
    "        log_sigma = self.log_sigma(h2)\n",
    "        theta = self.theta(x)\n",
    "        B = self.B(x)\n",
    "        context = self.context(B)\n",
    "        \n",
    "        # combine outputs\n",
    "        dense = self.context(h_masked) + context.unsqueeze(1)\n",
    "        out0 = torch.sigmoid(dense)\n",
    "        out = out0 * mask[:, :, None] + 0.000001\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and optimizer\n",
    "content_model = CONTENT(N_VOCAB, EMBED_SIZE, N_HIDDEN, N_TOPICS, MAX_LENGTH)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(content_model.parameters(),lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (300x619 and 200x50)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[49], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontent_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mNUM_EPOCHS\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[27], line 29\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_loader, val_loader, n_epochs)\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m x, x_mask, y, lengths, eventLengths \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m     22\u001B[0m \u001B[38;5;250m            \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;124;03m                1. zero grad\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;124;03m                2. model forward\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;124;03m                5. optimizer step\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;124;03m            \"\"\"\u001B[39;00m\n\u001B[1;32m---> 29\u001B[0m             outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m#             print(\"outputs\",outputs.shape)\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m#             print(\"y\",y.shape)\u001B[39;00m\n\u001B[0;32m     32\u001B[0m             loss \u001B[38;5;241m=\u001B[39m criterion(outputs, y) \n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[47], line 86\u001B[0m, in \u001B[0;36mCONTENT.forward\u001B[1;34m(self, x, mask)\u001B[0m\n\u001B[0;32m     84\u001B[0m mu \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmu(h2)\n\u001B[0;32m     85\u001B[0m log_sigma \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_sigma(h2)\n\u001B[1;32m---> 86\u001B[0m theta \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtheta\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     87\u001B[0m B \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mB(x)\n\u001B[0;32m     88\u001B[0m context \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext(B)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[47], line 28\u001B[0m, in \u001B[0;36mThetaLayer.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 28\u001B[0m     logsigma_in \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogsigma\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m     mu_in \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmu(x[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m     30\u001B[0m     kltermFn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m logsigma, mu: \u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m logsigma \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m-\u001B[39m (mu \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m-\u001B[39m torch\u001B[38;5;241m.\u001B[39mexp(logsigma) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (300x619 and 200x50)"
     ]
    }
   ],
   "source": [
    "train(content_model, train_loader, val_loader, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, accuracy_score\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# define the training function\n",
    "def train(model, optimizer, criterion, x, y, mask):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x, mask)\n",
    "    loss = criterion(output.flatten(), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# define the function to compute cost\n",
    "def compute_cost(model, x, y, mask):\n",
    "    output = model(x, mask)\n",
    "    loss = nn.BCELoss()(output.flatten(), y)\n",
    "    return loss.item()\n",
    "\n",
    "# define the function to output the hidden state\n",
    "def output_theta(model, x, mask):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        output, hn = model.rnn(x)\n",
    "        output = output * mask.unsqueeze(-1)\n",
    "        output = output.sum(dim=1) / mask.sum(dim=1).unsqueeze(-1)\n",
    "    return hn.detach().numpy().reshape(x.shape[0], -1), output.detach().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}