{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from config import Config\n",
    "from patient_data_reader import PatientReader\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Number of units in the hidden (recurrent) layer\n",
    "N_HIDDEN = 200\n",
    "# Number of training sequences in each batch\n",
    "\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "# How often should we check the output?\n",
    "EPOCH_SIZE = 100\n",
    "# Number of epochs to train the net\n",
    "num_epochs = 6\n",
    "\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "# FLAGS.vocab_size = 619\n",
    "\n",
    "\n",
    "\n",
    "N_BATCH = 128 # 128, 32, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(seqs, labels, vocabsize, maxlen=None):\n",
    "    \"\"\"Create the matrices from the datasets.\n",
    "\n",
    "    This pad each sequence to the same lenght: the lenght of the\n",
    "    longuest sequence or maxlen.\n",
    "\n",
    "    if maxlen is set, we will cut all sequence to this maximum\n",
    "    lenght.\n",
    "\n",
    "    This swap the axis!\n",
    "    \"\"\"\n",
    "    # x: a list of sentences\n",
    "    lengths = [len(s) for s in seqs]\n",
    "\n",
    "    eventSeq = []\n",
    "\n",
    "    for seq in seqs:\n",
    "        t = []\n",
    "        for visit in seq:\n",
    "            t.extend(visit)\n",
    "        eventSeq.append(t)\n",
    "    eventLengths = [len(s) for s in eventSeq]\n",
    "\n",
    "    if maxlen is not None:\n",
    "        new_seqs = []\n",
    "        new_lengths = []\n",
    "        new_labels = []\n",
    "        for l, s, la in zip(lengths, seqs, labels):\n",
    "            if l < maxlen:\n",
    "                new_seqs.append(s)\n",
    "                new_lengths.append(l)\n",
    "                new_labels.append(la)\n",
    "            else:\n",
    "                new_seqs.append(s[:maxlen])\n",
    "                new_lengths.append(maxlen)\n",
    "                new_labels.append(la[:maxlen])\n",
    "        lengths = new_lengths\n",
    "        seqs = new_seqs\n",
    "        labels = new_labels\n",
    "\n",
    "        if len(lengths) < 1:\n",
    "            return None, None, None\n",
    "\n",
    "    n_samples = len(seqs)\n",
    "    maxlen = max(maxlen, np.max(lengths)) # changed this line to always to goto max_len as we use in pytroch with batches\n",
    "\n",
    "    x = np.zeros((n_samples, maxlen, vocabsize)).astype('int64')\n",
    "    x_mask = np.zeros((n_samples, maxlen)).astype(float)\n",
    "    y = np.ones((n_samples, maxlen)).astype(float)\n",
    "    for idx, s in enumerate(seqs):\n",
    "        x_mask[idx, :lengths[idx]] = 1\n",
    "        for j, sj in enumerate(s):\n",
    "            for tsj in sj:\n",
    "                x[idx, j, tsj - 1] = 1\n",
    "    for idx, t in enumerate(labels):\n",
    "        y[idx, :lengths[idx]] = t\n",
    "        # if lengths[idx] < maxlen:\n",
    "        #     y[idx,lengths[idx]:] = t[-1]\n",
    "    \n",
    "#     # randomly generated list of labels. for testing. note that this size is n_samples,1 and not n_samples,n_visits\n",
    "#     y = torch.randint(0, 2, (n_samples,)) #.astype(float)\n",
    "    return x, x_mask, y, lengths, eventLengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] load resource\\vocab.pkl\n",
      " [*] load resource/X_train.pkl\n",
      " [*] load resource/Y_train.pkl\n",
      " [*] load resource/X_valid.pkl\n",
      " [*] load resource/Y_valid.pkl\n",
      " [*] load resource/X_test.pkl\n",
      " [*] load resource/Y_test.pkl\n",
      "vocabulary size: 619\n",
      "number of training documents: 2000\n",
      "number of validation documents: 500\n",
      "number of testing documents: 500\n",
      "legth of dataset of dtype = train: 2000\n",
      "legth of dataset of dtype = valid: 500\n",
      "legth of dataset of dtype = test: 500\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, seqs, hfs):\n",
    "        self.x = seqs\n",
    "        self.y = hfs\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    \n",
    "FLAGS = Config()\n",
    "data_sets = PatientReader(FLAGS)\n",
    "\n",
    "def get_custom_dataset(dtype):\n",
    "    \"\"\" dtype in train, valid, test\"\"\"\n",
    "    X_raw_data, Y_raw_data = data_sets.get_data_from_type(dtype)\n",
    "    dataset = CustomDataset(X_raw_data, Y_raw_data)\n",
    "    print(f\"legth of dataset of dtype = {dtype}:\", len(X_raw_data))\n",
    "    return dataset\n",
    "\n",
    "train_dataset = get_custom_dataset(\"train\")\n",
    "val_dataset = get_custom_dataset(\"valid\")\n",
    "test_dataset =get_custom_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "     Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "\n",
    "    x, x_mask, y, lengths, eventLengths = prepare_data(seqs=sequences, labels=labels, vocabsize=FLAGS.vocab_size, maxlen=MAX_LENGTH)\n",
    "    \n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    x_mask = torch.tensor(x_mask, dtype=torch.bool)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "    return x, x_mask, y, lengths, eventLengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data.dataset import random_split\n",
    "\n",
    "# split = int(len(dataset)*0.5)\n",
    "\n",
    "# lengths = [split, len(dataset) - split]\n",
    "# train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "# print(\"Length of train dataset:\", len(train_dataset))\n",
    "# print(\"Length of val dataset:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset,test_dataset, collate_fn,batch_size=128):\n",
    "    \n",
    "    '''\n",
    "    Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to batch_size. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        test dataset: test dataset of type `CustomDataset`\n",
    "        \n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader, test_dataset : train and validation and test dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset,test_dataset, collate_fn,batch_size = N_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "#     print(hidden_states.shape)  # torch.Size([32, 175, 256])\n",
    "\n",
    "#     print(masks.shape) #torch.Size([32, 175])\n",
    "#     \"\"\"\n",
    "#      obtain the hidden state for the last true visit (not padding visits)\n",
    "\n",
    "#     Arguments:\n",
    "#         hidden_states: the hidden states of each visit of shape (batch_size, # visits, embedding_dim)\n",
    "#         masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "#     Outputs:\n",
    "#         last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim)\n",
    "        \n",
    "#     NOTE: DO NOT use for loop.\n",
    "    \n",
    "#     HINT: First convert the mask to a vector of shape (batch_size,) containing the true visit length; \n",
    "#           and then use this length vector as index to select the last visit.\n",
    "#     \"\"\"\n",
    "\n",
    "    mask_length = masks.count_nonzero(dim=1)\n",
    "    return hidden_states[range(hidden_states.shape[0]),mask_length-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# input = torch.randn(batch_size, sequence_length, input_size)\n",
    "\n",
    "print_flag = False\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUModel, self).__init__()\n",
    "\n",
    "        self.gru = nn.GRU(input_size=FLAGS.vocab_size, hidden_size=N_HIDDEN, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features= N_HIDDEN, out_features=MAX_LENGTH)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, masks):\n",
    "        if print_flag: print( \"x\" ,x.shape)\n",
    "        if print_flag: print( \"masks\",masks.shape)\n",
    "        batch_size = x.shape[0]\n",
    "        if print_flag: print( \"batch_size\", batch_size)\n",
    "        output, h_n = self.gru(x)\n",
    "        if print_flag: print( \"output\", output.shape)\n",
    "        if print_flag: print( \"h_n\", h_n.shape)\n",
    "        true_h_n = get_last_visit(output, masks)\n",
    "        if print_flag: print( \"true_h_n\",true_h_n.shape)\n",
    "        logits = self.fc(true_h_n)   \n",
    "        if print_flag: print( \"logits\",logits.shape)\n",
    "        probs = self.sigmoid(logits)\n",
    "        if print_flag: print( \"probs\",probs.shape)\n",
    "        probs_ret = probs.view((batch_size,-1))\n",
    "        if print_flag: print( \"probs_ret\",probs_ret.shape)\n",
    "        return probs_ret\n",
    "    \n",
    "## H0 defaults to zeros if not provided.\n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(1, N_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUModel(\n",
       "  (gru): GRU(619, 200, batch_first=True)\n",
       "  (fc): Linear(in_features=200, out_features=300, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_rnn = GRUModel()\n",
    "naive_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 300, 619]), torch.Size([128, 300]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "x, x_mask, y, lengths, eventLengths = next(train_iter)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(naive_rnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "        \n",
    "    HINT: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, x_mask, y, lengths, eventLengths in data_loader:\n",
    "        y_hat = model(x, x_mask)\n",
    "        y_score = torch.cat((y_score,  y_hat.detach().to(device)), dim=0)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to(device)), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to(device)), dim=0)\n",
    "    \"\"\"\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "#     print(y_pred.shape, y_true.shape)\n",
    "    p, r, f, _ = precision_recall_fscore_support(torch.flatten(y_true), (np.array(y_pred)>0.5).flatten(), average='binary')\n",
    "    roc_auc = roc_auc_score(torch.flatten(y_true), torch.flatten(y_score))\n",
    "    return p, r, f, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        \n",
    "    You need to call `eval_model()` at the end of each training epoch to see how well the model performs \n",
    "    on validation data.\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, x_mask, y, lengths, eventLengths in train_loader:\n",
    "            \"\"\"\n",
    "                1. zero grad\n",
    "                2. model forward\n",
    "                3. calculate loss\n",
    "                4. loss backward\n",
    "                5. optimizer step\n",
    "            \"\"\"\n",
    "            outputs = model(x, x_mask)\n",
    "#             print(\"outputs\",outputs.shape)\n",
    "#             print(\"y\",y.shape)\n",
    "            loss = criterion(outputs, y) \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "              .format(epoch+1, p, r, f, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 0.559083\n",
      "Epoch: 1 \t Validation p: 0.91, r:0.92, f: 0.92, roc_auc: 0.90\n",
      "Epoch: 2 \t Training Loss: 0.285555\n",
      "Epoch: 2 \t Validation p: 0.91, r:0.92, f: 0.92, roc_auc: 0.92\n",
      "Epoch: 3 \t Training Loss: 0.273508\n",
      "Epoch: 3 \t Validation p: 0.91, r:0.92, f: 0.92, roc_auc: 0.92\n",
      "Epoch: 4 \t Training Loss: 0.271554\n",
      "Epoch: 4 \t Validation p: 0.91, r:0.92, f: 0.92, roc_auc: 0.92\n",
      "Epoch: 5 \t Training Loss: 0.270376\n",
      "Epoch: 5 \t Validation p: 0.92, r:0.92, f: 0.92, roc_auc: 0.93\n",
      "Epoch: 6 \t Training Loss: 0.269131\n",
      "Epoch: 6 \t Validation p: 0.92, r:0.92, f: 0.92, roc_auc: 0.93\n"
     ]
    }
   ],
   "source": [
    "n_epochs = num_epochs\n",
    "train(naive_rnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9260353617118455\n",
      "0.9255035281296328\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc = eval_model(naive_rnn, val_loader)\n",
    "print(roc_auc)\n",
    "assert roc_auc > 0.7, \"ROC AUC is too low on the validation set (%f < 0.7)\"%(roc_auc)\n",
    "\n",
    "\n",
    "p, r, f, roc_auc = eval_model(naive_rnn, test_loader)\n",
    "print(roc_auc)\n",
    "assert roc_auc > 0.7, \"ROC AUC is too low on the test set (%f < 0.7)\"%(roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9219502761966419,\n",
       " 0.9222526871772444,\n",
       " 0.9221014568923833,\n",
       " 0.9255035281296328)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, r, f, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognition Net  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RecognitionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecognitionNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(1473, 64)\n",
    "        \n",
    "        l_in = lasagne.layers.InputLayer(shape=(N_BATCH, MAX_LENGTH, N_VOCAB))\n",
    "        l_in = nn.Sequential(\n",
    "            nn.Linear(in_features=N_VOCAB, out_features=N_VOCAB),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=N_VOCAB, out_features=MAX_LENGTH * N_VOCAB),\n",
    "            nn.Reshape(N_BATCH, MAX_LENGTH, N_VOCAB)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CONTENT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONTENT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CONTENT, self).__init__()\n",
    "        \n",
    "        self.gru = GRUModel()\n",
    "        self.recog = RecognitionNet()\n",
    "        self.log_reg = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out = self.gru(x)\n",
    "        recog_out = self.recog(x)\n",
    "        z_out =  W * gru_out + B * recog_out\n",
    "        z_out = self.linear(z_out)\n",
    "        y_pred = torch.sigmoid(z_out)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lasagne' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m content_model \u001B[38;5;241m=\u001B[39m \u001B[43mCONTENT\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[29], line 6\u001B[0m, in \u001B[0;36mCONTENT.__init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28msuper\u001B[39m(CONTENT, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgru \u001B[38;5;241m=\u001B[39m GRUModel()\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecog \u001B[38;5;241m=\u001B[39m \u001B[43mRecognitionNet\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_reg \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(input_dim, output_dim)\n",
      "Cell \u001B[1;32mIn[24], line 7\u001B[0m, in \u001B[0;36mRecognitionNet.__init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28msuper\u001B[39m(RecognitionNet, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1 \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;241m1473\u001B[39m, \u001B[38;5;241m64\u001B[39m)\n\u001B[1;32m----> 7\u001B[0m l_in \u001B[38;5;241m=\u001B[39m \u001B[43mlasagne\u001B[49m\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mInputLayer(shape\u001B[38;5;241m=\u001B[39m(N_BATCH, MAX_LENGTH, N_VOCAB))\n\u001B[0;32m      8\u001B[0m l_in \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[0;32m      9\u001B[0m     nn\u001B[38;5;241m.\u001B[39mLinear(in_features\u001B[38;5;241m=\u001B[39mN_VOCAB, out_features\u001B[38;5;241m=\u001B[39mN_VOCAB),\n\u001B[0;32m     10\u001B[0m     nn\u001B[38;5;241m.\u001B[39mDropout(p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m),\n\u001B[0;32m     11\u001B[0m     nn\u001B[38;5;241m.\u001B[39mLinear(in_features\u001B[38;5;241m=\u001B[39mN_VOCAB, out_features\u001B[38;5;241m=\u001B[39mMAX_LENGTH \u001B[38;5;241m*\u001B[39m N_VOCAB),\n\u001B[0;32m     12\u001B[0m     nn\u001B[38;5;241m.\u001B[39mReshape(N_BATCH, MAX_LENGTH, N_VOCAB)\n\u001B[0;32m     13\u001B[0m )\n",
      "\u001B[1;31mNameError\u001B[0m: name 'lasagne' is not defined"
     ]
    }
   ],
   "source": [
    "content_model = CONTENT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mBCELoss()\n\u001B[0;32m      2\u001B[0m learning_rate \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.001\u001B[39m\n\u001B[1;32m----> 3\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(\u001B[43mcontent_model\u001B[49m\u001B[38;5;241m.\u001B[39mparameters(),lr\u001B[38;5;241m=\u001B[39mlearning_rate)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'content_model' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(content_model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}